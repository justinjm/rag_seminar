{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fe07e0-2bb8-4980-a5dd-1eee71c85ac3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This notebook contains solutions for the choose your own adventure notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11058015-d112-4d8d-881a-2726b272b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbbfa0-da8c-4fe5-84e0-bc722acf30c2",
   "metadata": {},
   "source": [
    "### Project and matching engine settings\n",
    "Note: copy these from notebook 01 for maximum efficiecy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344c9c1-0192-4417-ab78-7d6955a6107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "# PROJECT_ID = \"YOUR_PROJECT_HERE\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "ME_REGION = \"us-central1\"\n",
    "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"  # @param {type:\"string\"}\n",
    "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"  # @param {type:\"string\"}\n",
    "ME_DIMENSIONS = 768  # when using Vertex PaLM Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782df561-6677-4e17-bce2-b7445a95e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mengine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7e63a-7475-48f0-a8e1-103ca70be8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
    "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
    "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b50155-3f19-462b-aba0-d026e85bcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings object\n",
    "embeddings = VertexAIEmbeddings()\n",
    "# initialize vector store\n",
    "me = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=ME_REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bed4b1-b87e-4d09-8138-a831b8beda51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vector lookup\n",
    "Perform a direct lookup of the question \"What are video localized narratives?\" using k=2.  Take a look at https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.matching_engine.MatchingEngine.html#langchain.vectorstores.matching_engine.MatchingEngine.similarity_search for reference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd277bcc-ade2-44b1-8198-f31faf52663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95121a-a049-4423-9186-c23bdce18b31",
   "metadata": {},
   "source": [
    "### Building a retriever\n",
    "Next, build a retriever from your Vertex Vector Search, take a look at https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore for reference.  Hint: your vector store object is \"me\". Perform a retriever lookup of the same question - \"What are video localized narratives?\" using similarity search with 10 results and a score of 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08123ae4-0465-4eb1-a5fd-3c96700212ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe74a0-154d-4b72-8f3f-e2a39ad2881b",
   "metadata": {},
   "source": [
    "### Do a lookup against the retriever\n",
    "\n",
    "Use the same question, \"What are video localized narratives?\"\n",
    "\n",
    "What is different than the direct search against the vector store?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4a0f8-1d71-46f4-8a90-07d49eac7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaeeefd-30ef-4916-a099-14c90aae8459",
   "metadata": {},
   "source": [
    "### Create model objects\n",
    "Create a model object named \"text_bison\" using text-bison, with a temperature of 0.2, max output tokens of 1024, top_k of 40, and top_p of 0.95.  \n",
    "Hint 1: https://api.python.langchain.com/en/latest/llms/langchain.llms.vertexai.VertexAI.html, https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm\n",
    "Hint 2: You can pass arguments into VertexAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b079eb-a8af-4a3a-a706-620f8214ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3960a1-f6e9-4ec9-a8af-a35169cb7d34",
   "metadata": {},
   "source": [
    "### Build a basic LLMChain\n",
    "Using https://python.langchain.com/docs/modules/chains/foundational/llm_chain and the model object you created, ask text-bison what year Google went public and print the response.\n",
    "\n",
    "Hint: use a simple prompt template, don't overthink it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82aa81-c4ea-4f89-976f-bad752b35f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b212f1-5259-4667-bc1b-393656dba4d6",
   "metadata": {},
   "source": [
    "### Build a better prompt template\n",
    "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/\n",
    "\n",
    "Build a prompt template with two input variables, {question} and {context}.  {question} will be the question you're asking the model, and {context} will be the documents retrieved from the vector store.  Remember all of the thing you know about good prompt writing!\n",
    "\n",
    "It can be helpful to offset the context with delimiters like \"==========\" or to bound the context with pseudo-markdown.  Write the prompt template so that you're instructing the model to ONLY respond from the provided context to minimize\n",
    "hallucinations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f06007-1888-49f1-961f-4d873650d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here:\n",
    "\n",
    "prompt_template = \"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46002c89-08dc-44ab-a7bd-8e589afea2d0",
   "metadata": {},
   "source": [
    "### Build a QA chain\n",
    "\n",
    "Using the model object for text-bison and the prompt_template you created previously, create a RetrievalQA chain that combines the prompt, model, and retreiver you've already created.\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa.  \n",
    "\n",
    "Hint: Use the following parameters after setting the llm parameter:\n",
    "\n",
    "chain_type=\"stuff\",\n",
    "retriever=retriever,\n",
    "return_source_documents=True,\n",
    "verbose=False,\n",
    "chain_type_kwargs={\n",
    "    \"prompt\": PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    ),\n",
    "},\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b7a06-7190-472f-be9b-9f2d1f6c115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c3805-d51e-4516-861f-74f941333b07",
   "metadata": {},
   "source": [
    "### Ask the QA chain \n",
    "Ask the question \"What are video localized narratives?\"\n",
    "\n",
    "Hint: use similar syntax as when you called the LLMChain above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9e715-0b8e-4df4-80df-353bf980e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb23b2-911a-4558-a620-ce69607f2ca9",
   "metadata": {},
   "source": [
    "### Parse the response\n",
    "Print only the response to the question.  \n",
    "\n",
    "Hint: the dict key is \"result\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133e5f2-40e0-47ac-a52f-6616666edaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c720c8-4351-487e-8d79-8b5c41607aee",
   "metadata": {},
   "source": [
    "### Try and break the context\n",
    "\n",
    "Try different question combinations to see if you can break out of the prompt context.  Ask things like \"what is apple pie\" and \"how to I make a peanut butter and jelly sandwich?\"\n",
    "\n",
    "What happened, and why?\n",
    "\n",
    "If you weren't able to break out of your prompt, return to the prompt creation cell and edit the prompt until it tells you how to make a delicious peanut butter and jelly sandwich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d98c4b-f345-4250-b5b9-ee41c2b20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5eabf-f7fa-4969-9c1e-af3e66e402a6",
   "metadata": {},
   "source": [
    "### Implementing memory\n",
    "\n",
    "Create a ConversationBufferMemory object https://python.langchain.com/docs/modules/memory/types/buffer named \"memory\" with memory_key='chat_history', input_key='question', and output_key='answer'.\n",
    "\n",
    "Set return_messages to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891c773-a3af-4294-af6f-9067eea93ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2afe6-cf30-4b7c-9ee6-a90ae6ceccb6",
   "metadata": {},
   "source": [
    "### Creating a new prompt\n",
    "\n",
    "This one is a little challenging - create a new prompt using a ChatPromptTemplate.  Specify the context, chat_history, and question as variables in the prompt as the system template.\n",
    "\n",
    "Create another prompt as the user template with the variable question.\n",
    "\n",
    "Try and create a prompt maximally grounded in the context provided from the vector lookup.\n",
    "\n",
    "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e680005-cdee-4e4a-8ea1-cba602c9578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_system_template = \"\"\" \"\"\"\n",
    "general_user_template = \"\"\" \"\"\"\n",
    "messages = [\n",
    "            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "]\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119c359-6de0-418a-b5f1-4ca2b77939d5",
   "metadata": {},
   "source": [
    "### Create a ConversationalRetrievalChain\n",
    "\n",
    "Create a ConversationalRetrievalChain using the text_bison object as the llm and the following parameters:\n",
    "\n",
    "retriever=retriever,\n",
    "verbose=False, \n",
    "chain_type=\"stuff\",\n",
    "memory=memory,\n",
    "get_chat_history=lambda h : h,\n",
    "return_source_documents=True,\n",
    "combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/chat_vector_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c201f-8ca7-48bf-80d1-4590a909328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8bf2f-141f-4bd7-8836-6b20fae6030e",
   "metadata": {},
   "source": [
    "### Ask a question\n",
    "\n",
    "Ask conv_qa_chain the question, \"What are video localized narratives?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe12d0-c43b-4103-98f8-bdbd2c7decdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92b128-5e69-47c6-8f9d-afb29cace0b2",
   "metadata": {},
   "source": [
    "### Follow-up questions\n",
    "\n",
    "Ask a follow-up question, \"What do they empower?\"\n",
    "\n",
    "Make sure you take a look at the chat_history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd477b-d45f-42ea-81e1-1e23b9822ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b6a72-decd-4df8-9493-c15abf47fb1e",
   "metadata": {},
   "source": [
    "### Rebuild the conv_qa_chain and clear the memory\n",
    "Set verbose=True\n",
    "\n",
    "To clear memory, take a look at https://github.com/langchain-ai/langchain/issues/6585#issuecomment-1602935899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94823ae6-a795-4b5a-a6f8-60fcfe5256df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872c994-be73-4b2b-a39a-f8b5f8009799",
   "metadata": {},
   "source": [
    "### Ask the same question\n",
    "\n",
    "\"What are video localized narratives?\"\n",
    "\n",
    "Read through the (extremely verbose) response.  What is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc303b0-0c16-401a-931f-cea9d5148598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296c567-662a-4ede-b113-54a211697812",
   "metadata": {},
   "source": [
    "### Ask the same follow-up:\n",
    "\n",
    "Ask the follow-up question, \"What do they empower?\"\n",
    "\n",
    "Look very closely at the end of the chain where it says \"Human: Question:\" - was the actual question submitted what you wrote?  Why or why not?  Hint: https://python.langchain.com/docs/use_cases/question_answering/how_to/chat_vector_db#using-a-different-model-for-condensing-the-question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45eeb3-78de-4a4e-b2e9-f100d7230ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb12c7-64b1-461d-85c6-1576a205ddaa",
   "metadata": {},
   "source": [
    "### Future/bonus content\n",
    "\n",
    "Consider how you might use https://www.gradio.app/ or https://streamlit.io/ to build demos for customers.  \n",
    "\n",
    "Note - using them in a Vertex notebook is not a great experience, you're usually better off using local development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab50e4-8c00-4e76-8be7-cea73eb9097d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
