{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fe07e0-2bb8-4980-a5dd-1eee71c85ac3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This notebook contains solutions for the choose your own adventure notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11058015-d112-4d8d-881a-2726b272b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbbfa0-da8c-4fe5-84e0-bc722acf30c2",
   "metadata": {},
   "source": [
    "### Project and matching engine settings\n",
    "Note: copy these from notebook 01 for maximum efficiecy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344c9c1-0192-4417-ab78-7d6955a6107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "# PROJECT_ID = \"YOUR_PROJECT_HERE\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "ME_REGION = \"us-central1\"\n",
    "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"  # @param {type:\"string\"}\n",
    "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"  # @param {type:\"string\"}\n",
    "ME_DIMENSIONS = 768  # when using Vertex PaLM Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782df561-6677-4e17-bce2-b7445a95e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mengine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7e63a-7475-48f0-a8e1-103ca70be8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
    "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
    "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b50155-3f19-462b-aba0-d026e85bcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings object\n",
    "embeddings = VertexAIEmbeddings()\n",
    "# initialize vector store\n",
    "me = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=ME_REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bed4b1-b87e-4d09-8138-a831b8beda51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vector lookup\n",
    "Perform a direct lookup of the question \"What are video localized narratives?\" using k=2.  Take a look at https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.matching_engine.MatchingEngine.html#langchain.vectorstores.matching_engine.MatchingEngine.similarity_search for reference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd277bcc-ade2-44b1-8198-f31faf52663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "me.similarity_search(\"What are video localized narratives?\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95121a-a049-4423-9186-c23bdce18b31",
   "metadata": {},
   "source": [
    "### Building a retriever\n",
    "Next, build a retriever from your Vertex Vector Search, take a look at https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore for reference.  Hint: your vector store object is \"me\". Perform a retriever lookup of the same question - \"What are video localized narratives?\" using similarity search with 10 results and a score of 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08123ae4-0465-4eb1-a5fd-3c96700212ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose index to the retriever\n",
    "retriever = me.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,\n",
    "        \"search_distance\": 0.6,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75512f93-88ef-4a34-9976-1f6485e830f7",
   "metadata": {},
   "source": [
    "### Do a lookup against the retriever\n",
    "\n",
    "What is different than the direct search against the vector store?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4a0f8-1d71-46f4-8a90-07d49eac7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retriever.get_relevant_documents(\"What are video localized narratives?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8da3a-8e79-4423-ad21-34cef227f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose index to the retriever\n",
    "mmr_ret = me.as_retriever(search_type=\"mmr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333e8f1-45b2-48b4-b05f-9590ba7ddc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mmr_ret.get_relevant_documents(\"What are video localized narratives?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaeeefd-30ef-4916-a099-14c90aae8459",
   "metadata": {},
   "source": [
    "### Create model objects\n",
    "Create a model object named \"text_bison\" using text-bison, with a temperature of 0.2, max output tokens of 1024, top_k of 40, and top_p of 0.95.  \n",
    "Hint 1: https://api.python.langchain.com/en/latest/llms/langchain.llms.vertexai.VertexAI.html, https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm\n",
    "Hint 2: You can pass arguments into VertexAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b079eb-a8af-4a3a-a706-620f8214ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "text_bison = VertexAI(\n",
    "    temperature=0.2,\n",
    "    model_name='text-bison@001',\n",
    "    max_output_tokens=1024,\n",
    "    top_k=40,\n",
    "    top_p=.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3960a1-f6e9-4ec9-a8af-a35169cb7d34",
   "metadata": {},
   "source": [
    "### Build a basic LLMChain\n",
    "Using https://python.langchain.com/docs/modules/chains/foundational/llm_chain and the model object you created, ask text-bison what year Google went public and print the response.\n",
    "\n",
    "Hint: use a simple prompt template, don't overthink it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82aa81-c4ea-4f89-976f-bad752b35f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "\n",
    "prompt_template = \"Answer the {question}\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=text_bison,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "llm_chain(\"What year did Google go public?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b212f1-5259-4667-bc1b-393656dba4d6",
   "metadata": {},
   "source": [
    "### Build a better prompt template\n",
    "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/\n",
    "\n",
    "Build a prompt template with two input variables, {question} and {context}.  {question} will be the question you're asking the model, and {context} will be the documents retrieved from the vector store.  Remember all of the thing you know about good prompt writing!\n",
    "\n",
    "It can be helpful to offset the context with delimiters like \"==========\" or to bound the context with pseudo-markdown.  Write the prompt template so that you're instructing the model to ONLY respond from the provided context to minimize\n",
    "hallucinations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f06007-1888-49f1-961f-4d873650d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here:\n",
    "\n",
    "prompt_template = \"\"\"SYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
    "\n",
    "Do not try to make up an answer:\n",
    " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
    " - If the context is empty, just say \"I do not know the answer to that.\"\n",
    "\n",
    "=============\n",
    "{context}\n",
    "=============\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46002c89-08dc-44ab-a7bd-8e589afea2d0",
   "metadata": {},
   "source": [
    "### Build a QA chain\n",
    "\n",
    "Using the model object for text-bison and the prompt_template you created previously, create a RetrievalQA chain that combines the prompt, model, and retreiver you've already created.\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa.  \n",
    "\n",
    "For now, set verbose to False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b7a06-7190-472f-be9b-9f2d1f6c115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=text_bison,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c3805-d51e-4516-861f-74f941333b07",
   "metadata": {},
   "source": [
    "### Ask the QA chain \n",
    "Ask the question \"What are video localized narratives?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9e715-0b8e-4df4-80df-353bf980e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "\n",
    "qa(\"What are video localized narratives?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb23b2-911a-4558-a620-ce69607f2ca9",
   "metadata": {},
   "source": [
    "### Parse the response\n",
    "Print only the response to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133e5f2-40e0-47ac-a52f-6616666edaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "resp = qa(\"What are video localized narratives?\")\n",
    "print(resp['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c720c8-4351-487e-8d79-8b5c41607aee",
   "metadata": {},
   "source": [
    "### Try and break the context\n",
    "\n",
    "Try different question combinations to see if you can break out of the prompt context.  Ask things like \"what is apple pie\" and \"how to I make a peanut butter and jelly sandwich?\"\n",
    "\n",
    "What happened, and why?\n",
    "\n",
    "If you weren't able to break out of your prompt, return to the prompt creation cell and edit the prompt until it tells you how to make a delicious peanut butter and jelly sandwich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d98c4b-f345-4250-b5b9-ee41c2b20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "resp = qa(\"how to I make a peanut butter and jelly sandwich?\")\n",
    "print(resp['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6be30-85a7-4375-bd6e-8a36f99cee13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af5eabf-f7fa-4969-9c1e-af3e66e402a6",
   "metadata": {},
   "source": [
    "### Implementing memory\n",
    "\n",
    "Create a ConversationBufferMemory object https://python.langchain.com/docs/modules/memory/types/buffer named \"memory\" with memory_key='chat_history', input_key='question', and output_key='answer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891c773-a3af-4294-af6f-9067eea93ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history', input_key='question', output_key='answer',return_messages=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2afe6-cf30-4b7c-9ee6-a90ae6ceccb6",
   "metadata": {},
   "source": [
    "### Creating a new prompt\n",
    "\n",
    "This one is a little challenging - create a new prompt using a ChatPromptTemplate.  Specify the context, chat_history, and question as variables in the prompt as the system template.\n",
    "\n",
    "Create another prompt as the user template with the variable question.\n",
    "\n",
    "Try and create a prompt maximally grounded in the context provided from the vector lookup.\n",
    "\n",
    "https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e680005-cdee-4e4a-8ea1-cba602c9578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_system_template = \"\"\" \n",
    "You are an intelligent assistant helping the users with their questions on research papers. Think step-by-step and then answer.\n",
    "Strictly use ONLY use the provided pieces of context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) for answers. \n",
    "\n",
    "Do not try to make up an answer:\n",
    " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
    " - If the context is empty, just say \"I do not know the answer to that.\"\n",
    " \n",
    "----\n",
    "<ctx>\n",
    "{context}\n",
    "</ctx>\n",
    "----\n",
    "<hs>\n",
    "{chat_history}\n",
    "</hs>\n",
    "----\n",
    "\"\"\"\n",
    "general_user_template = \"Question:```{question}```\"\n",
    "messages = [\n",
    "            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "]\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119c359-6de0-418a-b5f1-4ca2b77939d5",
   "metadata": {},
   "source": [
    "### Create a ConversationalRetrievalChain\n",
    "\n",
    "Create a ConversationalRetrievalChain using the text_bison object as the llm and the following parameters:\n",
    "\n",
    "retriever=retriever,\n",
    "verbose=False, \n",
    "chain_type=\"stuff\",\n",
    "memory=memory,\n",
    "get_chat_history=lambda h : h,\n",
    "return_source_documents=True,\n",
    "combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/chat_vector_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c201f-8ca7-48bf-80d1-4590a909328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=text_bison, \n",
    "        retriever=retriever,\n",
    "        verbose=False, \n",
    "        chain_type=\"stuff\",\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda h : h,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8bf2f-141f-4bd7-8836-6b20fae6030e",
   "metadata": {},
   "source": [
    "### Ask a question\n",
    "\n",
    "Ask conv_qa_chain the question, \"What are video localized narratives?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe12d0-c43b-4103-98f8-bdbd2c7decdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_qa_chain(\"What are video localized narratives?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92b128-5e69-47c6-8f9d-afb29cace0b2",
   "metadata": {},
   "source": [
    "### Follow-up questions\n",
    "\n",
    "Ask a follow-up question, \"What do they empower?\"\n",
    "\n",
    "Make sure you take a look at the chat_history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd477b-d45f-42ea-81e1-1e23b9822ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "conv_qa_chain(\"What do they empower?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b6a72-decd-4df8-9493-c15abf47fb1e",
   "metadata": {},
   "source": [
    "### Rebuild the conv_qa_chain and clear the memory\n",
    "Set verbose=True\n",
    "\n",
    "To clear memory, take a look at https://github.com/langchain-ai/langchain/issues/6585#issuecomment-1602935899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94823ae6-a795-4b5a-a6f8-60fcfe5256df",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=text_bison, \n",
    "        retriever=retriever,\n",
    "        verbose=True, \n",
    "        chain_type=\"stuff\",\n",
    "        memory=memory,\n",
    "        get_chat_history=lambda h : h,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
    "    )\n",
    "\n",
    "memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872c994-be73-4b2b-a39a-f8b5f8009799",
   "metadata": {},
   "source": [
    "### Ask the same question\n",
    "Read through the (extremely verbose) response.  What is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc303b0-0c16-401a-931f-cea9d5148598",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_qa_chain(\"What are video localized narratives?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296c567-662a-4ede-b113-54a211697812",
   "metadata": {},
   "source": [
    "### Ask the same follow-up:\n",
    "\n",
    "Ask the follow-up question, \"What do they empower?\"\n",
    "\n",
    "Look very closely at the end of the chain where it says \"Human: Question:\" - was the actual question submitted what you wrote?  Why or why not?  Hint: https://python.langchain.com/docs/use_cases/question_answering/how_to/chat_vector_db#using-a-different-model-for-condensing-the-question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45eeb3-78de-4a4e-b2e9-f100d7230ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "conv_qa_chain(\"What do they empower?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df249c-4f35-404c-8186-72d01f1e8088",
   "metadata": {},
   "source": [
    "### Future/bonus content\n",
    "\n",
    "Consider how you might use https://www.gradio.app/ or https://streamlit.io/ to build demos for customers.  \n",
    "\n",
    "Note - using them in a Vertex notebook is not a great experience, you're usually better off using local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684e258-f627-47f6-aee2-2f5871b4d08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
